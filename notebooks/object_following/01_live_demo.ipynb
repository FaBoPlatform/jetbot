{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Following - 物体追跡\n",
    "\n",
    "このノートブックでは、JetBotで物体を追跡する方法を示します。 collision avoidanceをベースに、「free(直進する)」時に物体を追跡します。  \n",
    "物体検出に使うモデルは一般的な90種類のオブジェクトの画像を分類した[COCOデータセット](http://cocodataset.org)を事前にトレーニングしたssd_mobilenet_v2モデルを利用します。  \n",
    "このモデルはTensorRTに変換したものを使用しますが、JetPackバージョンによってTensorRTのバージョンが異なるため、変換時のTensorRTバージョンと同一の実行環境である必要があります。\n",
    "\n",
    "追跡可能な物体はCOCOデータセットで学習している物体となります。\n",
    "\n",
    "* 人（インデックス1）\n",
    "* カップ（インデックス47）\n",
    "\n",
    "その他多数あります（クラスインデックスの完全なリストについては、[ラベルファイル](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt)で確認できます）。  \n",
    "インデックス0はbackgroundになります。通常、分類・検出するモデルでは「未検出」という状態を持つためにbackgroundラベルが使われています。  \n",
    "学習済みモデルは[Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)で公開されているものをベースに予めTensorRT化してあるものを使います。``Tensorflow Object Detection API``を使って自前のデータをデスクトップPCやクラウドサーバーで学習することも出来ます。\n",
    "\n",
    "ssd_mobilenet_v2_cocoをTensorRTに変換することにより、物体検出モデルの実行が非常に高速になり、Jetson Nanoでリアルタイムに実行できるようになります。ただし、このノートブックではCOCOデータセットからのトレーニングや他の最適化に関する手順は実行しません。また、TensorRTはバージョンによりAPIが頻繁に変更されているため、他のJetPackバージョンで動作していたssd_mobilenet_v2_coco.engineは利用できません。\n",
    "\n",
    "> このノートブックはJetson Nano 4GB JetPack 4.3で作られたJetBotで動作します。  \n",
    "> Jetson Nano 2GB (JetPack 4.4以降)とJetson Nano 4GB JetPack 4.4以降では動作確認していません。\n",
    "\n",
    "まずは始めてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カメラの準備\n",
    "カメラを初期化しましょう。物体検出モデルは300x300ピクセルの画像を入力とするため、カメラ解像度を300x300に設定します。\n",
    "\n",
    "> 内部的には、CameraクラスはGStreamerを使用してJetson Nanoのイメージシグナルプロセッサ（ISP）を利用しています。これはCPUでリサイズ処理を実行する場合とは比較にならないほど超高速です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "from jetbot import Camera  # JetBot用に用意したカメラライブラリを利用します。\n",
    "\n",
    "########################################\n",
    "# カメラを有効化します。\n",
    "# 画像はwidthとheightで指定したピクセルサイズにリサイズされます。\n",
    "# ssd_mobilenet_v2_cocoは300x300の入力層のため、カメラ画像は300x300にリサイズします。\n",
    "# fpsのデフォルトは21ですが、カメラフレーム更新に連動して推論を実行するようにコーディングしているため、\n",
    "# 処理が重くなってしまいます。そのためfpsを小さく設定します。\n",
    "########################################\n",
    "camera = Camera(width=300, height=300, fps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前トレーニング済みのSSDエンジンを使用する[ObjectDetector](https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/jetbot/object_detection.py)クラスをインポートして、``ssd_mobilenet_v2_coco.engine``をロードします。\n",
    "\n",
    "Jetpack4.3向けの[ssd_monbilenet_v2_coco.engine](https://drive.google.com/file/d/1KjlDMRD8uhgQmQK-nC2CZGHFTbq4qQQH/view) をダウンロードし、JupyterLabの本Notebookと同じフォルダに``ssd_monbilenet_v2_coco.engine``をアップロードします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD MobileNet V2モデルを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "from jetbot import ObjectDetector  # JetBot用に用意した物体検出ライブラリを利用します。\n",
    "\n",
    "########################################\n",
    "# TensorRTの物体検出モデルを読み込みます。\n",
    "########################################\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内部的には、``ObjectDetector``クラスはTensorRT Python APIを使用してモデルを実行します。また、モデルへの入力の前処理や、検出されたオブジェクトの解析も行います。 現時点では、``jetbot.ssd_tensorrt``パッケージを使用して作成されたモデルでのみ機能します。このパッケージには、モデルをTensorflowオブジェクト検出APIから最適化されたTensorRTエンジンに変換するためのユーティリティが含まれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、カメラ入力を使用してネットワークを実行してみましょう。 デフォルトでは ``ObjectDetector``クラスはカメラが生成する``bgr8``フォーマットを期待しています。 しかし、別のフォーマットを入力に使う場合は、デフォルトの前処理関数をオーバーライドして変更できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import cv2\n",
    "\n",
    "########################################\n",
    "# 物体検出はTensorflowで学習されたモデルを使っています。このモデルは学習時にRGB画像フォーマットで学習されています。\n",
    "# そのモデルをTensorRTモデルに変換したものがssd_mobilenet_v2_coco.engineです。\n",
    "# 物体検出モデルの入力データはRGBフォーマットに変換する方が精度がよくなりますが、\n",
    "# BGR->RGB変換をObjectDetectorクラスが実行しているため、\n",
    "# このノートブックにおける物体検出の入力データはOpenCVカメラ画像のBGRフォーマットのまま渡すことになります。\n",
    "########################################\n",
    "detections = model(camera.value)\n",
    "\n",
    "print(detections)  # 推論結果を表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カメラ画像にCOCOオブジェクトがある場合、その情報は``detections``変数に格納されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキスト領域に検出を表示する\n",
    "\n",
    "次のコードを使用して、検出されたオブジェクトの情報をテキストエリアに表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detections_widget = widgets.Textarea()  # テキストウィジェットを作成します。\n",
    "\n",
    "detections_widget.value = str(detections)  # テキストウィジェットに検出したオブジェクトの情報を反映します。\n",
    "\n",
    "display(detections_widget)  # テキストウィジェットを表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カメラ画像で検出された各オブジェクトのラベルID、信頼度、境界ボックスの座標が表示されます。\n",
    "\n",
    "ミニバッチ学習時に複数の画像を一度に学習したなごりで、予測時にも一度に複数の画像を入力として期待するモデルに仕上がっています。  \n",
    "今回は1台のカメラしか使わないため、モデルの入力には1枚の画像を持つ配列が使われています。  \n",
    "最初の画像で検出された最初のオブジェクトのみを表示するには、次のように呼び出すことができます。\n",
    "\n",
    "> オブジェクトが検出されない場合、エラーになるため、try-exceptでエラーハンドリングします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0  # 推論時に配列で与えた最初の画像を表す配列のインデックス番号。推論時は1枚の画像しか与えていないため、0固定値。\n",
    "object_number = 0  # 検出した物体の情報を持つ配列から、取り出したい配列のインデックス番号。複数の物体が検出された場合は0以外もあり得る。検出しなかった場合、配列は存在しない。\n",
    "\n",
    "try:\n",
    "    print(detections[image_number][object_number])\n",
    "except:\n",
    "    print(\"object not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中心物体を追跡するようにロボットを制御する\n",
    "\n",
    "次に、ロボットに指定されたクラスのオブジェクトを追跡させます。 これを行うには、次のようにします\n",
    "\n",
    "1.  指定したクラスに一致するオブジェクトを検出します。[ラベルファイル](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt)でラベルIDと対応する物体を確認してください。\n",
    "2.  カメラの視野の中心に最も近いオブジェクトを選択します。これが指定したオブジェクトの時に追跡するターゲットになります。\n",
    "3.  ロボットをターゲットオブジェクトに向けます。\n",
    "4.  collision avoidanceをベース動作にしているため、障害物によってブロックされていると判断した場合は、左折します。\n",
    "\n",
    "> ラベルファイルにはいくつかバージョンがあります。Tensorflowのラベルは80オブジェクト分になります。  \n",
    "そのため、いくつか名前のないラベルが含まれています。[cocoデータセットのラベルについて](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/)\n",
    "\n",
    "また、ターゲットオブジェクトのラベル、ロボットの速度を制御するために使用するいくつかのウィジェットを作成します。\n",
    "`turn gain`は、ターゲットオブジェクトとロボットの視野の中心との間の距離に基づいてロボットが回転する速度を制御します。\n",
    "\n",
    "まず、衝突回避モデルをロードします。\n",
    "衝突回避の例に従って、実際の環境でうまく動作するモデルを使用することをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "########################################\n",
    "# 衝突回避モデルを読み込みます。\n",
    "########################################\n",
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "collision_model.load_state_dict(torch.load('../collision_avoidance/best_model.pth'))\n",
    "\n",
    "########################################\n",
    "# GPU処理が可能な部分をGPUで処理するように設定します。\n",
    "# モデルを評価モードにします。\n",
    "# モデルをfloat16型に変換します。\n",
    "########################################\n",
    "device = torch.device('cuda')\n",
    "collision_model = collision_model.to(device)\n",
    "collision_model = collision_model.eval().half()\n",
    "\n",
    "########################################\n",
    "# この値はpytorch ImageNetの学習に使われた正規化（ImageNetデータセットのRGB毎に平均を0、標準偏差が1になるようにスケーリングすること）のパラメータです。\n",
    "# カメラ画像はこの値でRGBを正規化することが望ましいでしょう。\n",
    "# ここではtransforms.ToTensor()を使っていないため、正規化前のRGB値の範囲は[0, 255]です。\n",
    "# そこで、学習時のRGB各範囲と同じ範囲にスケーリングするように正規化パラメータに255.0を掛けて設定します。\n",
    "########################################\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "########################################\n",
    "# 正規化する関数を定義します。\n",
    "# torchvision.transforms.Normalizeクラスはインスタンス化すると\n",
    "# torch.nn.functional.normalize関数を返します。\n",
    "# ソースコード：\n",
    "#   https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html#Normalize\n",
    "#   https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#normalize\n",
    "#########################################\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "########################################\n",
    "# カメラ画像をモデル入力用データに変換します。\n",
    "########################################\n",
    "def preprocess(camera_value):\n",
    "    # OpenCVで取得したカメラ画像を変数xにコピーします。\n",
    "    x = camera_value\n",
    "    # 画像解像度を300x300から224x224に変更します。\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    # 学習時の画像データはtorchvision.datasets.ImageFolderを使って読み込んでいるため、モデルはRGBフォーマットの画像で学習しています。\n",
    "    # カメラ映像はOpenCVで読み込んでいるため画像はBGRフォーマットになっています。これをRGBフォーマットに変換します。    \n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    # 画像フォーマットをHWCからCHWに変換します。\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    # float32に変換します。\n",
    "    x = torch.from_numpy(x).float()\n",
    "    # 正規化します。\n",
    "    x = normalize(x)\n",
    "    # GPUデバイスを利用します。float16に変換します。\n",
    "    x = x.to(device).half()\n",
    "    # バッチ配列に変換します。\n",
    "    x = x[None, ...]\n",
    "    # 入力用データxを返します。\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モーターを制御するためにrobotインスタンスを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "from jetbot import Robot  # JetBotを制御するためのライブラリを利用します。\n",
    "\n",
    "########################################\n",
    "# JetBotの制御用クラスをインスタンス化します。\n",
    "########################################\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コントロールウィジェットとカメラ更新とモデル実行の関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "from jetbot import bgr8_to_jpeg  # JetBot用に用意した画像変換ライブラリを利用します。\n",
    "\n",
    "########################################\n",
    "# 「blocked」の確率を表示するためのスライダーを用意します。\n",
    "########################################\n",
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "\n",
    "########################################\n",
    "# 画像表示用のウィジェットを用意します。\n",
    "# widthとheightは表示するウィジェットの幅と高さです。\n",
    "# カメラ画像サイズと一致する必要はありません。\n",
    "########################################\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "\n",
    "########################################\n",
    "# 追跡対象のラベル名を選択するためのウィジェットを作成します。\n",
    "# ラベル名は学習済みモデルssd_mobilenet_v2_coco.engineが持つラベル名になります。\n",
    "# 追跡対象はpersonとしておきます。\n",
    "########################################\n",
    "label_widget = widgets.Dropdown(\n",
    "    options=['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "             'fire hydrant', '12', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "             'cow', 'elephant', 'bear', 'zebra', 'giraffe', '26', 'backpack', 'umbrella', '29', '30',\n",
    "             'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "             'skateboard', 'surfboard', 'tennis racket', 'bottle', '45', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "             'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "             'cake', 'chair', 'couch', 'potted plant', 'bed', '66', 'dining table', '68', '69', 'toilet',\n",
    "             '71', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "             'sink', 'refrigerator', '83', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'],\n",
    "    value='person',\n",
    "    description='tracked label',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "########################################\n",
    "# JetBotの動作を調整するためのスライダーウィジェットを用意します。\n",
    "########################################\n",
    "speed_widget = widgets.FloatSlider(value=0.0, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "########################################\n",
    "# ウィジェットの画像サイズを取得しておきます。\n",
    "########################################\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "########################################\n",
    "# 描画する文字のサイズを自動調整します。\n",
    "########################################\n",
    "fontScale = height/1000.0\n",
    "if fontScale < 0.4:\n",
    "    fontScale = 0.4\n",
    "fontThickness = 1 + int(fontScale)\n",
    "fontFace = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "########################################\n",
    "# カメラ画像の中央を原点とした、\n",
    "# 検出した追跡対象の中央座標(center_x, center_y)を取得します。\n",
    "########################################\n",
    "def detection_center(detection):\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "########################################\n",
    "# カメラ画像の中央と追跡対象の中央までの距離を取得します。\n",
    "########################################\n",
    "def norm(vec):\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "########################################\n",
    "# 複数の追跡対象ターゲットのうち、もっとも画面中央に映っているターゲットを取得します。\n",
    "########################################\n",
    "def closest_detection(detections):\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "\n",
    "########################################\n",
    "# カメラ画像が更新されたときに実行する処理を定義します。\n",
    "########################################\n",
    "def execute(change):\n",
    "    # カメラ画像を変数imageにコピーします。\n",
    "    image = change['new']\n",
    "        \n",
    "    ####################\n",
    "    # 衝突回避モデルを実行して、「blocked」かどうかを判断します。\n",
    "    ####################\n",
    "    # 推論を実行します。\n",
    "    collision_output = collision_model(preprocess(image))\n",
    "    # collision_output.flatten()を呼び出すことで可能な限り不要な次元を除去します。([[blocked_rate, free_rate]]を[blocked_rate, free_rate]に変換)\n",
    "    # softmax()関数を適用して出力ベクトルの合計が1になるように正規化します（これにより確率分布になります）\n",
    "    # 入力データは多次元のバッチ配列になっています。出力もそれに対応しているためcollision_output.flatten()は多次元配列になっています。\n",
    "    # そのうえで、「blocked」の確率となるcollision_output.flatten()[0]の値を取得します。「free」の確率を取得する場合はcollision_output.flatten()[1]になります。\n",
    "    prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    # 「blocked」の確率をスライダーに反映します。\n",
    "    blocked_widget.value = prob_blocked\n",
    "    \n",
    "    ####################\n",
    "    # 「blocked」の確率が50%未満なら直進します。\n",
    "    # 画像表示ウィジェットを更新します。\n",
    "    # この関数の処理をここで終了します。\n",
    "    ####################\n",
    "    if prob_blocked > 0.5:\n",
    "        robot.left(0.3)\n",
    "        image_widget.value = bgr8_to_jpeg(image)\n",
    "        return\n",
    "    \n",
    "    ####################\n",
    "    # 「blocked」の確率が50%以下なら、つまり「free」なら物体検出を実行します。\n",
    "    ####################\n",
    "    # 物体検出モデルの実行コード内でBGR->RGB変換をおこなっているため、\n",
    "    # ここではOpenCVカメラ画像のBGRフォーマットのまま渡します。\n",
    "    detections = model(image)\n",
    "    \n",
    "    # 検出した物体の情報を表示します。\n",
    "    display_str = []\n",
    "    display_str.append(\"detection info\")\n",
    "    for det in detections[0]:  # 検出した物体を一つ一つ解析します。\n",
    "        if det['label']  == 0:  # 検出結果のうち、ラベル番号0は背景のためスキップします。\n",
    "            # background. skip\n",
    "            continue\n",
    "        if det['confidence'] <= 0.2:  # スコアが低い場合。ここでは確認のために検出したものとしてpassします。\n",
    "            # bad score. skip\n",
    "            #continue\n",
    "            pass\n",
    "        bbox = det['bbox']  # 検出した物体の範囲を表す長方形のx,y座標を取得します。\n",
    "        score = det['confidence']  # 検出した物体のスコア（確率）を取得します。\n",
    "        label = det['label']  # 検出した物体のラベル番号を取得します。\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)  # 検出した物体を青色の長方形で囲みます。\n",
    "        display_str.append(\"label:{} score:{:.2f}\".format(label_widget.options[int(label)-1], score))  # ラベル名とスコアを文字列の配列に追加します。\n",
    "        #cv2.putText(image, display_str, org=(10, 20+20*num_detection), fontFace=fontFace, fontScale=fontScale, thickness=fontThickness, color=(77, 255, 9))\n",
    "\n",
    "    ####################\n",
    "    # 検出した物体のラベル名とスコアを画像に描画します。\n",
    "    # 描画位置やパディングは画像サイズと文字数から、見やすくなるように計算して描画します。\n",
    "    ####################\n",
    "    max_text_width = 0\n",
    "    max_text_height = 0\n",
    "    if len(display_str) > 0:\n",
    "        [(text_width, text_height), baseLine] = cv2.getTextSize(text=display_str[0], fontFace=fontFace, fontScale=fontScale, thickness=fontThickness)\n",
    "        x_left = int(baseLine)\n",
    "        y_top = int(baseLine)\n",
    "        for i in range(len(display_str)):\n",
    "            [(text_width, text_height), baseLine] = cv2.getTextSize(text=display_str[i], fontFace=fontFace, fontScale=fontScale, thickness=fontThickness)\n",
    "            if max_text_width < text_width:\n",
    "                max_text_width = text_width\n",
    "            if max_text_height < text_height:\n",
    "                max_text_height = text_height\n",
    "        for i in range(len(display_str)):\n",
    "            cv2.putText(image, display_str[i], org=(x_left, y_top + int(max_text_height*1.2 + (max_text_height*1.2 * i))), fontFace=fontFace, fontScale=fontScale, thickness=fontThickness, color=(77, 255, 9))\n",
    "\n",
    "    ####################\n",
    "    # 検出した物体が追跡対象のラベルと一致している場合、その情報を取得します。\n",
    "    # ラベル番号は、物体検出の結果は0が背景、1が「person」ラベルになります。\n",
    "    # ラベル選択ウィジェットのドロップダウンリストの配列は背景を選択しないようにしているため、\n",
    "    # 0が「person」ラベルになります。そのため、ラベル選択ウィジェットのインデックスに+1したものが物体検出のラベル番号と一致することになります。\n",
    "    ####################\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.index)+1]\n",
    "    \n",
    "    ####################\n",
    "    # 追跡対象の物体のうち、画面中央にもっとも近い物体を追跡対象とします。\n",
    "    ####################\n",
    "    target = closest_detection(matching_detections)\n",
    "    \n",
    "    ####################\n",
    "    # 追跡対象となる物体が存在する場合は、物体を緑色の長方形で囲みます。\n",
    "    ####################\n",
    "    if target is not None:\n",
    "        bbox = target['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "\n",
    "    ####################\n",
    "    # 追跡対象となる物体が存在しない場合は、衝突回避の「free」と同じように前進します。\n",
    "    ####################\n",
    "    if target is None:\n",
    "        robot.forward(float(speed_widget.value))\n",
    "        \n",
    "    ####################\n",
    "    # 追跡対象となる物体が存在する場合は、物体の中心方向を向くようにモーターを制御します。\n",
    "    ####################\n",
    "    else:\n",
    "        center = detection_center(target)\n",
    "        robot.set_motors(\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0])\n",
    "        )\n",
    "    \n",
    "    ####################\n",
    "    # 画像表示ウィジェットを更新します。\n",
    "    ####################\n",
    "    image_widget.value = bgr8_to_jpeg(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル推論からJetBotの動作までを実行する関数を作成しました。  \n",
    "今度はそれをカメラ画像の更新に連動して動作させる必要があります。\n",
    "\n",
    "JetBotでは、traitlets.HasTraitsを継承したCameraクラスを実装しているので、observe()を呼び出すだけで実現できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JetBotを動かしてみよう\n",
    "次のコードで``start jetbot``ボタンと``stop jetbot``ボタンを作成します。  \n",
    "``start jetbot``ボタンを押すとモデルの初期化が実行され、JetBotが動作し始めます。  \n",
    "``stop jetbot``ボタンを押すとJetBotが停止します。  \n",
    "最初の1フレームの実行時にメモリの初期化が実行されるので、ディープラーニングではどんなモデルも最初の1フレームの処理はすこし時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import ipywidgets\n",
    "import time\n",
    "\n",
    "########################################\n",
    "# スタートボタンとストップボタンを作成します。\n",
    "########################################\n",
    "model_start_button = ipywidgets.Button(description='start jetbot')\n",
    "model_stop_button = ipywidgets.Button(description='stop jetbot')\n",
    "\n",
    "########################################\n",
    "# スタートボタンがクリックされた時に呼び出す関数を定義します。\n",
    "########################################\n",
    "def start_model(c):\n",
    "    execute({'new': camera.value})  # execute()関数を1回呼び出して初期化します。\n",
    "    camera.observe(execute, names='value')  # Cameraクラスのtraitlets.Any()型のvalue変数(カメラ画像データ)が更新されたときに指定した関数を呼び出します。\n",
    "model_start_button.on_click(start_model)  # startボタンがクリックされた時に指定した関数を呼び出します。\n",
    "\n",
    "########################################\n",
    "# ストップボタンがクリックされた時に呼び出す関数を定義します。\n",
    "########################################\n",
    "def stop_model(c):\n",
    "    camera.unobserve(execute, names='value')  # カメラ画像データの更新と指定した関数の連動を解除します。\n",
    "    time.sleep(1)  # 実行中の処理が完了するまで少し待ちます。\n",
    "    robot.stop()  # モーターを停止します。\n",
    "model_stop_button.on_click(stop_model)  # stopボタンがクリックされた時に指定した関数を呼び出します。\n",
    "\n",
    "########################################\n",
    "# ウィジェットの表示レイアウトを定義します。\n",
    "########################################\n",
    "model_widget = ipywidgets.VBox([\n",
    "    image_widget,\n",
    "    ipywidgets.HBox([label_widget, blocked_widget]),\n",
    "    ipywidgets.HBox([speed_widget, turn_gain_widget]),\n",
    "    ipywidgets.HBox([model_start_button, model_stop_button])\n",
    "])\n",
    "\n",
    "########################################\n",
    "# ウィジェットを表示します。\n",
    "########################################\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "うごいた！  \n",
    "ターゲットが検出されると緑色のボックスが表示され、ターゲット以外の検出された物体は青色のボックスで表示されます。  \n",
    "衝突回避モデルによって「blocked(旋回する)」と判断された時、JetBotは左に曲がります。  \n",
    "衝突回避モデルによって「free(直進する)」と判断された時、ターゲットを検出している場合はJetBotはターゲットを追跡するように動作します。  \n",
    "衝突回避モデルによって「free(直進する)」と判断された時、ターゲットを検出していない場合は衝突回避モデルと同様に直進します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カメラの停止\n",
    "最後に、他のノートブックでカメラを使うために、このノートブックで使ったカメラを停止しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()  # カメラを停止します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
