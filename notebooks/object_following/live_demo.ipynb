{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Following - Live Demo\n",
    "\n",
    "このノートブックでは、JetBotで物体を追跡する方法を示します。 collision avoidanceをベースに、「free(直進する)」時に物体を追跡します。\\\n",
    "物体検出に使うモデルは一般的な90種類のオブジェクトの画像を分類した[COCOデータセット](http://cocodataset.org)を事前にトレーニングしたssd_mobilenet_v2モデルを利用します。\\\n",
    "このモデルはTensorRTに変換したものを使用しますが、JetPackバージョンによってTensorRTのバージョンが異なるため、変換時のTensorRTバージョンと同一の実行環境である必要があります。\n",
    "\n",
    "追跡可能な物体はCOCOデータセットで学習している物体となります。\n",
    "\n",
    "* 人（インデックス1）\n",
    "* カップ（インデックス47）\n",
    "\n",
    "その他多数あります（クラスインデックスの完全なリストについては、[ラベルファイル](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt)で確認できます）。\\\n",
    "インデックス0はbackgroundになります。通常、分類・検出するモデルでは「未検出」という状態を持つためにbackgroundラベルが使われています。\\\n",
    "学習済みモデルは[Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)で公開されているものをベースに予めTensorRT化してあるものを使います。``Tensorflow Object Detection API``を使って自前のデータをデスクトップPCやクラウドサーバーで学習することも出来ます。\n",
    "\n",
    "ssd_mobilenet_v2_cocoをTensorRTに変換することにより、物体検出モデルの実行が非常に高速になり、Jetson Nanoでリアルタイムに実行できるようになります。ただし、このノートブックではCOCOデータセットからのトレーニングや他の最適化に関する手順は実行しません。また、TensorRTはバージョンによりAPIが頻繁に変更されているため、他のJetPackバージョンで動作していたssd_mobilenet_v2_coco.engineは利用できません。\n",
    "\n",
    "まずは始めてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Live Camera (カメラの準備)\n",
    "本サンプルを実行するにあたり、まずnvargus-daemon(カメラ等で使用)をリスタートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo jetbot | sudo -S systemctl restart nvargus-daemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、カメラを初期化しましょう。物体検出モデルは300x300ピクセルの画像を入力とするため、カメラ解像度を300x300に設定します。\n",
    "\n",
    "> 内部的には、CameraクラスはGStreamerを使用してJetson Nanoのイメージシグナルプロセッサ（ISP）を利用しています。これはCPUでリサイズ処理を実行する場合とは比較にならないほど超高速です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300, fps=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前トレーニング済みのSSDエンジンを使用する[ObjectDetector](https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/jetbot/object_detection.py)クラスをインポートして、ssd_mobilenet_v2_coco.engineをロードします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD MobileNet V2モデルを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内部的には、``ObjectDetector``クラスはTensorRT Python APIを使用してモデルを実行します。また、モデルへの入力の前処理や、検出されたオブジェクトの解析も行います。 現時点では、``jetbot.ssd_tensorrt``パッケージを使用して作成されたモデルでのみ機能します。このパッケージには、モデルをTensorflowオブジェクト検出APIから最適化されたTensorRTエンジンに変換するためのユーティリティが含まれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、カメラ入力を使用してネットワークを実行してみましょう。 デフォルトでは ``ObjectDetector``クラスはカメラが生成する``bgr8``フォーマットを期待しています。 しかし、別のフォーマットを入力に使う場合は、デフォルトの前処理関数をオーバーライドして変更できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = model(camera.value)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カメラ画像にCOCOオブジェクトがある場合、その情報は``detections``変数に格納されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキスト領域に検出を表示する\n",
    "\n",
    "次のコードを使用して、検出されたオブジェクトの情報をテキストエリアに表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detections_widget = widgets.Textarea()\n",
    "\n",
    "detections_widget.value = str(detections)\n",
    "\n",
    "display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カメラ画像で検出された各オブジェクトのラベルID、信頼度、境界ボックスの座標が表示されます。\n",
    "\n",
    "ミニバッチ学習時に複数の画像を一度に学習したなごりで、予測時にも一度に複数の画像を入力として期待するモデルに仕上がっています。\\\n",
    "今回は1台のカメラしか使わないため、モデルの入力には1枚の画像を持つ配列が使われています。\\\n",
    "最初の画像で検出された最初のオブジェクトのみを表示するには、次のように呼び出すことができます。\n",
    "\n",
    "> オブジェクトが検出されない場合、エラーになるため、try-exceptでエラーハンドリングします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0\n",
    "object_number = 0\n",
    "\n",
    "try:\n",
    "    print(detections[image_number][object_number])\n",
    "except:\n",
    "    print(\"object not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中心物体を追跡するようにロボットを制御する\n",
    "\n",
    "次に、ロボットに指定されたクラスのオブジェクトを追跡させます。 これを行うには、次のようにします\n",
    "\n",
    "1.  指定したクラスに一致するオブジェクトを検出します。[ラベルファイル](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt)でラベルIDと対応する物体を確認してください。\n",
    "2.  カメラの視野の中心に最も近いオブジェクトを選択します。これが指定したオブジェクトの時に追跡するターゲットになります。\n",
    "3.  ロボットをターゲットオブジェクトに向けます。\n",
    "4.  collision avoidanceをベース動作にしているため、障害物によってブロックされていると判断した場合は、左折します。\n",
    "\n",
    "> ラベルファイルにはいくつかバージョンがあります。Tensorflowのラベルは80オブジェクト分になります。\\\n",
    "そのため、いくつか名前のないラベルが含まれています。[cocoデータセットのラベルについて](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/)\n",
    "\n",
    "また、ターゲットオブジェクトのラベル、ロボットの速度を制御するために使用するいくつかのウィジェットを作成します。\n",
    "`turn gain`は、ターゲットオブジェクトとロボットの視野の中心との間の距離に基づいてロボットが回転する速度を制御します。\n",
    "\n",
    "まず、衝突回避モデルをロードします。\n",
    "衝突回避の例に従って、実際の環境でうまく動作するモデルを使用することをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "collision_model.load_state_dict(torch.load('../collision_avoidance/best_model.pth'))\n",
    "device = torch.device('cuda')\n",
    "collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、ロボットを初期化して、モーターを制御できるようにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コントロールウィジェットとカメラ更新とモデル実行の関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.Dropdown(\n",
    "    options=['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "             'fire hydrant', '12', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "             'cow', 'elephant', 'bear', 'zebra', 'giraffe', '26', 'backpack', 'umbrella', '29', '30',\n",
    "             'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "             'skateboard', 'surfboard', 'tennis racket', 'bottle', '45', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "             'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "             'cake', 'chair', 'couch', 'potted plant', 'bed', '66', 'dining table', '68', '69', 'toilet',\n",
    "             '71', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "             'sink', 'refrigerator', '83', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'],\n",
    "    value='person',\n",
    "    description='tracked label',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "speed_widget = widgets.FloatSlider(value=0.0, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "\"\"\"font settings\"\"\"\n",
    "fontScale = height/1000.0\n",
    "if fontScale < 0.4:\n",
    "    fontScale = 0.4\n",
    "fontThickness = 1 + int(fontScale)\n",
    "fontFace = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "        \n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "    \n",
    "    # execute collision model to determine if blocked\n",
    "    collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    blocked_widget.value = prob_blocked\n",
    "    \n",
    "    # turn left if blocked\n",
    "    if prob_blocked > 1.1:\n",
    "        robot.left(0.3)\n",
    "        image_widget.value = bgr8_to_jpeg(image)\n",
    "        return\n",
    "        \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "    \n",
    "    # draw all detections on image\n",
    "    display_str = []\n",
    "    display_str.append(\"detection info\")\n",
    "    for det in detections[0]:\n",
    "        if det['label']  == 0:\n",
    "            # background. skip\n",
    "            #continue\n",
    "            pass\n",
    "        if det['confidence'] <= 0.2:\n",
    "            # bad score. skip\n",
    "            #continue\n",
    "            pass\n",
    "        bbox = det['bbox']\n",
    "        score = det['confidence']\n",
    "        label = det['label']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "        \"\"\"get text info\"\"\"\n",
    "        display_str.append(\"label:{} score:{:.2f}\".format(label_widget.options[int(label)+1], score))\n",
    "        #cv2.putText(image, display_str, org=(10, 20+20*num_detection), fontFace=fontFace, fontScale=fontScale, thickness=fontThickness, color=(77, 255, 9))\n",
    "\n",
    "    \"\"\"draw detection info\"\"\"\n",
    "    max_text_width = 0\n",
    "    max_text_height = 0\n",
    "    if len(display_str) > 0:\n",
    "        [(text_width, text_height), baseLine] = cv2.getTextSize(text=display_str[0], fontFace=fontFace, fontScale=fontScale, thickness=fontThickness)\n",
    "        x_left = int(baseLine)\n",
    "        y_top = int(baseLine)\n",
    "        for i in range(len(display_str)):\n",
    "            [(text_width, text_height), baseLine] = cv2.getTextSize(text=display_str[i], fontFace=fontFace, fontScale=fontScale, thickness=fontThickness)\n",
    "            if max_text_width < text_width:\n",
    "                max_text_width = text_width\n",
    "            if max_text_height < text_height:\n",
    "                max_text_height = text_height\n",
    "        for i in range(len(display_str)):\n",
    "            cv2.putText(image, display_str[i], org=(x_left, y_top + int(max_text_height*1.2 + (max_text_height*1.2 * i))), fontFace=fontFace, fontScale=fontScale, thickness=fontThickness, color=(77, 255, 9))\n",
    "\n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.index)+1]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    target = closest_detection(matching_detections)\n",
    "    if target is not None:\n",
    "        bbox = target['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "\n",
    "\n",
    "    # otherwise go forward if no target detected\n",
    "    if target is None:\n",
    "        robot.forward(float(speed_widget.value))\n",
    "        \n",
    "    # otherwsie steer towards target\n",
    "    else:\n",
    "        # move robot forward and steer proportional target's x-distance from center\n",
    "        center = detection_center(target)\n",
    "        robot.set_motors(\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0])\n",
    "        )\n",
    "    \n",
    "    # update image widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``start jetbot``ボタンを押すことでJetBotが動作するようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time\n",
    "\n",
    "model_start_button = ipywidgets.Button(description='start jetbot')\n",
    "model_stop_button = ipywidgets.Button(description='stop jetbot')\n",
    "\n",
    "model_widget = ipywidgets.VBox([\n",
    "    image_widget,\n",
    "    ipywidgets.HBox([label_widget, blocked_widget]),\n",
    "    ipywidgets.HBox([speed_widget, turn_gain_widget]),\n",
    "    ipywidgets.HBox([model_start_button, model_stop_button])\n",
    "])\n",
    "\n",
    "display(model_widget)\n",
    "\n",
    "\n",
    "def start_model(c):\n",
    "    execute({'new': camera.value})\n",
    "    camera.unobserve_all()\n",
    "    camera.observe(execute, names='value')\n",
    "model_start_button.on_click(start_model)\n",
    "    \n",
    "def stop_model(c):\n",
    "    camera.unobserve(execute, names='value')\n",
    "    #camera.unobserve_all()\n",
    "    time.sleep(1)\n",
    "    robot.stop()\n",
    "model_stop_button.on_click(stop_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "うごいた！\\\n",
    "ターゲットが検出されると緑色のボックスが表示され、ターゲット以外の検出された物体は青色のボックスで表示されます。\\\n",
    "衝突回避モデルによって「blocked(旋回する)」と判断された時、JetBotは左に曲がります。\\\n",
    "衝突回避モデルによって「free(直進する)」と判断された時、ターゲットを検出している場合はJetBotはターゲットを追跡するように動作します。\\\n",
    "衝突回避モデルによって「free(直進する)」と判断された時、ターゲットを検出していない場合は衝突回避モデルと同様に直進します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
