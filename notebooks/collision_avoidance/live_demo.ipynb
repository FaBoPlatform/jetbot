{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collision Avoidance - Live Demo(デモ)\n",
    "\n",
    "このnotebookでは、ロボットが「フリー」か「ブロック」かを検出するトレーニング済みモデルを使用して、ロボットの衝突回避動作を有効にします。\n",
    "\n",
    "## Load the trained model(学習済みモデルの読み込み)\n",
    "\n",
    "ローカルの学習なので、すでに``best_model.pth``がローカルに存在している事を前提とします。\n",
    "\n",
    "> Please make sure the file has uploaded fully before calling the next cell\n",
    "\n",
    "Pytorch modelの初期化を下記コードでおこないます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.alexnet(pretrained=False)\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、``best_model.pth``から、学習した重みを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在、モデルの重みは、CPUメモリーで実行されているので、下記コードでGPUに転送します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the preprocessing function(事前処理関数の作成)\n",
    "\n",
    "モデルを読み込みましたが、まだわずかな問題があります。学習済みモデルのフォーマットと、カメラの形式と*完全に*一致しません。これを解消するために、\n",
    "いくつかの*前処理*を行う必要があります。これらは、下記の手順になります。\n",
    "\n",
    "1. BGRからRGBに変換\n",
    "2. HWC layoutからCHW layoutに変換\n",
    "3. トレーニング中に使ったのと同じパラメーターを使用して正規化します（カメラは[0、255]の範囲の値を提供し、ロードされた画像は[0、1]の範囲でトレーニングするため、255.0でスケーリングする必要があります\n",
    "4. dataをCPUメモリからGPUメモリに転送します\n",
    "5. バッチディメンションを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すばらしい、これでカメラ形式からneural networkのインプットのフォーマットに変換するための、pre-processing関数を定義する事ができました。\n",
    "\n",
    "カメラを起動し、画面に表示します。カメラの起動には、大分なれてきましたね。また、ロボットがブロックされる確率が表示されるスライダーを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo jetbot | sudo -S systemctl restart nvargus-daemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224, fps=2)\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.HBox([image, blocked_slider]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モーターを制御するためにrobotインスタンスを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、カメラの値が変更されるたびに呼び出される関数を作成します。この関数は下記手順で実行します。\n",
    "\n",
    "1. カメラのイメージのPre-process\n",
    "2. neural networkの実行\n",
    "3. ニューラルネットワークの出力がブロックを示した場合は、左に曲がります。それ以外の場合は前進します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "def update(change):\n",
    "    global blocked_slider, robot\n",
    "    x = change['new'] \n",
    "    x = preprocess(x)\n",
    "    y = model(x)\n",
    "    \n",
    "    # we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    \n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    \n",
    "    blocked_slider.value = prob_blocked\n",
    "    \n",
    "    if prob_blocked < 0.5:\n",
    "        robot.forward(0.4)\n",
    "    else:\n",
    "        robot.left(0.4)\n",
    "    \n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すばらしい、neural networkの実行関数が生成できたので、あとは処理をカメラと関連づける必要がある。\n",
    "\n",
    "それは `` observe``関数で実現します。\n",
    "\n",
    "> 注意: このコードでロボットが動きだします。十分なスペースを確保してください。collision avoidanceが動くでしょう、nueral networkは、訓練されたデータと同じくらいに動くはずです。\n",
    "\n",
    "``start jetbot``でカメラのupdateがおこなわれ、`` observe``関数で処理をカメラと関連づけます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import time\n",
    "\n",
    "model_start_button = ipywidgets.Button(description='start jetbot')\n",
    "model_stop_button = ipywidgets.Button(description='stop jetbot')\n",
    "\n",
    "def start_model(c):\n",
    "    update({'new': camera.value})  \n",
    "    camera.observe(update, names='value')\n",
    "model_start_button.on_click(start_model)\n",
    "    \n",
    "def stop_model(c):\n",
    "    camera.unobserve(update, names='value')\n",
    "    time.sleep(1)\n",
    "    robot.stop()\n",
    "    #robot.stop()\n",
    "    #camera_link.unlink()\n",
    "model_stop_button.on_click(stop_model)\n",
    "\n",
    "model_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([image, blocked_slider]),\n",
    "    ipywidgets.HBox([model_start_button, model_stop_button])\n",
    "])\n",
    "\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驚くばかり！ロボットが接続されている場合、新しいカメラフレームごとに新しいコマンドが生成されるはずです。おそらく、ロボットを地面に置き、障害物に到達したときの動作を確認する事ができるでしょう。\n",
    "\n",
    "もし、今の挙動を止めたい場合は、下記コードを実行し、このコールバックの関連付けをやめます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "\n",
    "#camera.unobserve(update, names='value')\n",
    "\n",
    "#time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "\n",
    "#robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もしかしたら、videoをブラウザに転送しないでロボットを実行したいと思いますが、その場合は、下記のようにcameraをunlinkしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera_link.unlink()  # don't stream to browser (will still run camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "streamingを続ける場合は、下記コードで続けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera_link.link()  # stream to browser (wont run camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robotを停止します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "以上がLive demoです。うまく行けば、ロボットが衝突をインテリジェントに回避できたことと思います。\n",
    "\n",
    "avoiding collisionsが上手く行かない場合、失敗した箇所を見つけてください。美しさは、これら上手くいかないシナリオについてより多くのデータを収集すれば、ロボットはさらに良くなるはずです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
