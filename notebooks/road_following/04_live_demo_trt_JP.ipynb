{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following - 道路に沿った走行のデモ(TensorRT版)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorchで学習したモデルをTensorRTモデルに変換したことで高速処理が可能になりました。  \n",
    "このノートブックでは、TensorRT化したモデルを使うことでカクツキを抑えてJetBotがなめらかに走行することを確認できます。  \n",
    "Jetson Nano 4GBではこの効果が大きくなりますが、Jetson Nano 2GBではこのTensorRT版でなければまともに走行できないでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# デバイスの準備\n",
    "\n",
    "カメラ画像をGPUメモリに転送するために、先に定義だけしておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import torch\n",
    "\n",
    "########################################\n",
    "# TensorRTの場合、モデルはGPUを使うように実装されていますが、\n",
    "# 入力データはGPUメモリに転送する必要があります。\n",
    "# そのための定義をここでしておきます。\n",
    "########################################\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRTに最適化されたモデル``best_steering_model_xy_trt.pth``を読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import torch  # これはすでに読込んでいるため、省略可能です。\n",
    "from torch2trt import TRTModule  # TensorRTのライブラリを利用します。\n",
    "\n",
    "########################################\n",
    "# TensorRTモデルを読み込みます。\n",
    "########################################\n",
    "model_trt = TRTModule()  # TensorRTモデルを読み込むための変数を定義します。\n",
    "model_trt.load_state_dict(torch.load('best_steering_model_xy_trt.pth'))  # 学習したTensorRTモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カメラ画像の前処理作成\n",
    "モデルを読み込みましたが、まだ少し問題があります。  \n",
    "学習時の入力画像フォーマットと、OpenCVのカメラ画像フォーマットは一致しません。  \n",
    "これを解消するために、 いくつかの前処理を行う必要があります。これらは、下記の手順になります。\n",
    "\n",
    "1. HWCをCHWに変換します。\n",
    "> cudnnはHWC(Height x Width x Channel)をサポートしません。そのため画像情報の並び順をHWCからCHW(Channel x Height x Width)に変換します。  \n",
    "> これはto_tensor()で実行されます。\n",
    "2. カメラ画像を正規化します。\n",
    "> これは先に実行されたto_tensor()でRGB値が[0, 255]から[0.0, 1.0]に変換されています。そこにImageNetと同じ正規化パラメータを使ってPytorchのTensorクラスの引き算、割り算機能で正規化しています。\n",
    "3. カメラ画像をGPUメモリに転送します。\n",
    "> 入力データはモデルと同じデバイスに存在する必要があります。ここはto_tensor()の行でついでにto(device)という形で実施されています。また、half()でfloat16に変換されているので若干の高速化がされています。\n",
    "4. 入力画像データを配列に変更します。\n",
    "> 学習時はバッチサイズ分の画像を配列にして入力層に与えています。モデルの入力層は学習ために入力データを可変長の配列で受け取る構造になっています。そのため予測時は1枚の画像であっても入力画像データを配列にする必要があります。\n",
    "\n",
    "ここを見たとき、あなたはcollision avoidanceの時と何か違うことに気付くかもしれません。  \n",
    "えぇ、そうです。カメラ画像はOpenCV経由で取得したものなのでBGRですが、それをRGBに変換していません。  \n",
    "road followingでは学習時にBGRフォーマットで学習しているため、モデルの入力はBGRのままで最高のパフォーマンスを得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "########################################\n",
    "# この値はpytorch ImageNetの学習に使われた正規化（ImageNetデータセットのRGB毎に平均を0、標準偏差が1になるようにスケーリングすること）のパラメータです。\n",
    "# カメラ画像はこの値でRGBを正規化することが望ましいでしょう。\n",
    "########################################\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "########################################\n",
    "# カメラ画像をモデル入力用データに変換します。\n",
    "########################################\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)  # OpenCV画像データ(配列データ）をPILイメージオブジェクトに変換します。\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()  # 画像をTensor型に変換してfloat16型でGPUメモリに転送します。\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])  # ImageNetのパラメータで正規化します。\n",
    "    return image[None, ...]  # バッチ配列に変換して返します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> すばらしい、これでカメラ画像をニューラルネットワークの入力フォーマットに変換するための、pre-processing関数を定義できました。　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カメラの動作確認\n",
    "それでは、カメラを起動して表示しましょう。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "########################################\n",
    "# カメラを有効化します。\n",
    "# 画像はwidthとheightで指定したピクセルサイズにリサイズされます。\n",
    "# fpsのデフォルトは21ですが、カメラフレーム更新に連動して推論を実行するようにコーディングしているため、\n",
    "# 処理が重くなってしまいます。そのためfpsを小さく設定します。\n",
    "########################################\n",
    "camera = Camera(width=224, height=224, fps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = ipywidgets.Image()  # 画像表示用のウィジェットを用意します。\n",
    "\n",
    "########################################\n",
    "# traitletsライブラリを利用してカメラ画像データが更新されたときに、\n",
    "# bgr8フォーマットをjpegフォーマットに変換してから\n",
    "# 画像表示ウィジェットに反映するように設定します。\n",
    "########################################\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(image_widget)  # 画像表示ウィジェットをブラウザに表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モーター制御に必要なrobotインスタンスを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "from jetbot import Robot\n",
    "\n",
    "########################################\n",
    "# JetBotの制御用クラスをインスタンス化します。\n",
    "########################################\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " JetBotの動作パラメータを設定するためのsliderを作成します。\n",
    "> メモ: テスト時にうまく動作した値でスライダー値を初期化していますが、あなたのデータセットではうまく機能しない可能性があります。そのため、必要に応じてスライダーを増減してセットアップしてください。\n",
    "\n",
    "## PID制御パラメータ用のスライダーを作成します。\n",
    "1. ``speed gain``：スピードコントローラー。この値を増やすとJetBotのモーター出力が増加します。\n",
    "2. ``steering gain``：ステアリングゲインコントローラー。JetBotが左右にブレて不安定な場合は、スムーズに走るようにこの値を減らす必要があります。\n",
    "3. ``steering kd``：ステアリングディゲインコントローラー。JetBotが左右にブレて不安定な場合は、スムーズに走るようにこの値を増やす必要があります（元の角度を保つパラメータ）\n",
    "4. ``steering bias``：ステアリングバイアスコントローラー。JetBotがコースの右端または左端に偏って走行する場合は、JetBotが中央のラインまたはコースをたどるまでこのスライダーの値を調整します。ここでは、モーターバイアスと同様にカメラのoffsetも考慮します。\n",
    "\n",
    "> 注：JetBotの走行をスムーズにするために、上記のスライダーを少しずつ調整します。\n",
    "\n",
    "* PID制御について\n",
    "  * https://ja.wikipedia.org/wiki/PID%E5%88%B6%E5%BE%A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# JetBotの動作を調整するためのスライダーウィジェットを用意します。\n",
    "########################################\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.2, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "\n",
    "########################################\n",
    "# スライダーウィジェットをブラウザに表示します。\n",
    "########################################\n",
    "display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、JetBotがどう判断しているかを表示するためにいくつかのスライダーを画面に表示しましょう。xおよびyスライダーには、推論されたx, yの値を表示します。\n",
    "\n",
    "steeringスライダーは推定したステアリング値を表示します。この値はターゲットの実際の角度ではなく、ほぼ比例した単純な値です。  \n",
    "値が``0``の場合は真っ直ぐ進むことを意味します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 推論結果とJetBotのステアリング値および速度を表示するためのスライダーを用意します。\n",
    "########################################\n",
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "########################################\n",
    "# スライダーウィジェットをブラウザに表示します。\n",
    "########################################\n",
    "display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、カメラの画像が変わるたびに呼び出される関数を生成します。この関数は、下記ステップを実行します。\n",
    "\n",
    "1. カメラ画像をPre-processingにかけてモデル入力データに変換する\n",
    "2. モデル推論の実行\n",
    "3. ステアリング値を計算する\n",
    "4. 比例/微分制御（PD）を使用してモーターを制御する\n",
    "\n",
    "* Tensor型からnumpy.ndarray型に変換する前に、detach()が必要な理由\n",
    "  * https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# ステアリング計算用の変数を定義します。\n",
    "########################################\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "\n",
    "########################################\n",
    "# カメラ画像が更新されたときに実行する処理を定義します。\n",
    "########################################\n",
    "def execute(change):\n",
    "    # execute()関数はメインスレッドで動作します。このglobal宣言された変数は同じメインスレッドで定義されているため、省略可能です。\n",
    "    # execute()関数をサブスレッドから呼び出す場合はglobal宣言が必要になります。\n",
    "    global angle, angle_last\n",
    "    # カメラ画像を変数xにコピーします。\n",
    "    image = change['new']\n",
    "    \n",
    "    ####################\n",
    "    # TensorRTモデルで推論を実行して、xとyの値を取得します。\n",
    "    # カメラ画像をpreprocess(image)で入力用データに変換します。\n",
    "    # TensorRTモデルで推論を実行します。出力x,yはTensor型に格納されて返ってきます。\n",
    "    # 数値としてnumpyで扱いたいため、Tensor型からグラフレイヤーを削除するためにdetach()を実行します。\n",
    "    # （TRTModuleはtorch.nn.Moduleを継承しているため、このdetach()はtorch.nn.Module.detach()になります。）\n",
    "    # float32型に変換します。\n",
    "    # 値をCPUメモリに値を転送します。\n",
    "    # numpy.ndarray()型に変換します。\n",
    "    # numpy.flatten()でx,yの値を1次配列に変換します。\n",
    "    ####################\n",
    "    xy = model_trt(preprocess(image)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]  # xの値を取得します。\n",
    "    y = (0.5 - xy[1]) / 2.0  # yの値を取得します。\n",
    "    \n",
    "    ####################\n",
    "    # x,yの値をスライダーに反映します。\n",
    "    ####################\n",
    "    x_slider.value = x\n",
    "    y_slider.value = y\n",
    "    \n",
    "    ####################\n",
    "    # スピードゲインはそのままスピードとしてスライダーに反映します。\n",
    "    ####################\n",
    "    speed_slider.value = speed_gain_slider.value\n",
    "    \n",
    "    ####################\n",
    "    # ステアリング制御のためのPID制御パラメータを求めます。\n",
    "    ####################\n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle\n",
    "    \n",
    "    ####################\n",
    "    # ステアリング値をスライダーに反映します。\n",
    "    ####################\n",
    "    steering_slider.value = pid + steering_bias_slider.value\n",
    "    \n",
    "    ####################\n",
    "    # 左右モーターの出力を決定します。\n",
    "    ####################\n",
    "    robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "    \n",
    "execute({'new': camera.value})  # execute()関数を1回呼び出して初期化します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル推論からJetBotの動作までを実行する関数を作成しました。  \n",
    "今度はそれをカメラ画像の更新に連動して動作させる必要があります。\n",
    "\n",
    "JetBotでは、traitlets.HasTraitsを継承したCameraクラスを実装しているので、observe()を呼び出すだけで実現できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JetBotを動かしてみよう\n",
    "次のコードで``start jetbot``ボタンと``stop jetbot``ボタンを作成します。  \n",
    "``start jetbot``ボタンを押すとモデルの初期化が実行され、JetBotが動作し始めます。  \n",
    "``speed gain``を0から少しずつ値を大きくし、前進させます。  \n",
    "``steering gain``でハンドルの切れ角を調整します。0に近いほど、切れ角がゆるくなります。  \n",
    "最初の1フレームの実行時にメモリの初期化が実行されるので、ディープラーニングではどんなモデルも最初の1フレームの処理はすこし時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 利用するライブラリを読み込みます。\n",
    "########################################\n",
    "import ipywidgets\n",
    "import time\n",
    "\n",
    "########################################\n",
    "# スタートボタンとストップボタンを作成します。\n",
    "########################################\n",
    "model_start_button = ipywidgets.Button(description='start jetbot')\n",
    "model_stop_button = ipywidgets.Button(description='stop jetbot')\n",
    "\n",
    "########################################\n",
    "# スタートボタンがクリックされた時に呼び出す関数を定義します。\n",
    "########################################\n",
    "def start_model(c):\n",
    "    camera.observe(execute, names='value')  # Cameraクラスのtraitlets.Any()型のvalue変数(カメラ画像データ)が更新されたときに指定した関数を呼び出します。\n",
    "model_start_button.on_click(start_model)  # startボタンがクリックされた時に指定した関数を呼び出します。\n",
    "\n",
    "########################################\n",
    "# ストップボタンがクリックされた時に呼び出す関数を定義します。\n",
    "########################################    \n",
    "def stop_model(c):\n",
    "    camera.unobserve(execute, names='value')  # カメラ画像データの更新と指定した関数の連動を解除します。\n",
    "    time.sleep(1)  # 実行中の処理が完了するまで少し待ちます。\n",
    "    robot.stop()  # モーターを停止します。\n",
    "model_stop_button.on_click(stop_model)  # stopボタンがクリックされた時に指定した関数を呼び出します。\n",
    "\n",
    "########################################\n",
    "# ウィジェットの表示レイアウトを定義します。\n",
    "########################################\n",
    "model_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([speed_gain_slider, steering_gain_slider]),\n",
    "    ipywidgets.HBox([image_widget]),\n",
    "    steering_slider,\n",
    "    ipywidgets.HBox([model_start_button, model_stop_button])\n",
    "])\n",
    "\n",
    "########################################\n",
    "# ウィジェットを表示します。\n",
    "########################################\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カメラの停止\n",
    "データを取り終えたら、他のノートブックでカメラを使用できるようにカメラの接続を適切に閉じましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()  # カメラを停止します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結論\n",
    "以上がTensorRTのライブデモです。今はJetBotがコース上をなめらかに走行しているのではないでしょうか？　\n",
    "\n",
    "road followingが上手く行かない場合、正しく走行できるように失敗しやすい場所でさらにデータを追加してください。  \n",
    "このようにうまくいかない場所を中心にデータを収集すれば、JetBotはさらによく動作するはずです。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
