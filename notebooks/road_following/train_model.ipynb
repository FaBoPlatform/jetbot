{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following - モデルの学習(resnet18モデルの学習)\n",
    "\n",
    "このnotebookは、入力画像を読み込み、ターゲットに対応するx、y値のセットを出力するようにニューラルネットワークを学習します。\n",
    "\n",
    "road followingではtorchvisionで用意されているResNet18モデルを少し変更して使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Instance(データセットインスタンスの作成)\n",
    "\n",
    "[torch.utils.data.DataLoader](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py)クラスを継承して``__len__`` と ``__getitem__``関数を実装実装した``XYDataset``クラスを作成します。このクラスは、画像をロードするための役割と、画像ファイル名からx,y値の値をパースして取得します。[torch.utils.data.DataLoader](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py)を継承する事で、すべてのtorchデータユーティリティを使用する事ができます。\n",
    "\n",
    "いくつかの変換（カラージッターなど）をデータセットにハードコーディングしました。カラージッターは画像の明るさ、コントラスト、彩度をランダムに変更します。\\\n",
    "ランダムに水平反転を有効にするオプション``random_hflips``を付けました。デフォルトは有効にしています。水平反転は「右車線にとどまる」必要が無い場合、つまり左右どちらの車線を通ってもいい場合に有効にすることでデータセットの特性が変わります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(path):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path[3:6])) - 50.0) / 50.0\n",
    "\n",
    "def get_y(path):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path[7:10])) - 50.0) / 50.0\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.image_paths = glob.glob(os.path.join(directory, '*.jpg'))\n",
    "        self.random_hflips = random_hflips\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        x = float(get_x(os.path.basename(image_path)))\n",
    "        y = float(get_y(os.path.basename(image_path)))\n",
    "        # ランダムに画像を水平反転する時、出力のxも対応するように反転する\n",
    "        if self.random_hflips:\n",
    "            if float(np.random.rand(1)) > 0.5:\n",
    "                image = transforms.functional.hflip(image)\n",
    "                x = -x\n",
    "\n",
    "        # 画像をモデル学習の入力用データフォーマットに変換する\n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        # ImageNetの正規化と同じパラメータでデータを正規化する\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset('dataset_xy', random_hflips=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test sets(トレーニングデータとテストデータに分ける)\n",
    "次に、データセットを*トレーニング用*と*テスト用*のデータセットに分割します。この例では、*トレーニング用*に90%, *テスト用*に10%で分けます。*テスト用*のデータセットは、学習中にモデルの精度を検証するために使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders to load data in batches(バッチ処理で学習データとテストデータを読み込むためのデータローダーを作成)\n",
    "\n",
    "[torch.utils.data.DataLoader](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py)クラスは、モデル学習中に次のデータ処理が完了出来るようにサブプロセスで並列処理にして実装します。\\\n",
    "データのシャッフル、バッチでのデータロードのために使用します。この例では、1回のバッチ処理で8枚の画像を使用します。これをバッチサイズと呼び、GPUのメモリ使用量と、モデルの精度に影響を与えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network Model (JetBot用にモデルを変更する)\n",
    "\n",
    "torchvisionで使用可能なImageNetデータセットで学習済みのResNet18モデルを使用します。\n",
    "\n",
    "*転移学習*と呼ばれる手法で、すでに画像分類できる特徴を持つニューラルネットワーク層を、別の目的のために作られたモデルに適用することで、短時間で良好な結果を得られるモデルを作成することができます。\n",
    "\n",
    "ResNet18の詳細: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "転移学習の詳細：https://www.youtube.com/watch?v=yofjFQddwHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet18モデルはImageNetを学習するために作られているため、1000種類の画像分類が可能な出力を持っています。ResNet18モデル構造の全結合層(fully connected layer)を入れ替えて、JetBotで欲しい出力x,yの2種類を得られるモデル構造にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc.in_features)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デフォルトではモデルのweightはCPUで処理されるため、GPUを利用するようにモデルを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before: {}\".format(model.fc.weight.type()))\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "print(\"after: {}\".format(model.fc.weight.type()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning(ファインチューニング)\n",
    "初めての学習では、この項目はスキップして**「視覚化ユーティリティ」**に進んでください。\\\n",
    "学習データが500件を超えて、学習時間が遅くなってきた時にファインチューニングが役立ちます。\n",
    "\n",
    "学習データが800枚にもなると、学習に必要なメモリ容量が増えてきてSWAPが多く消費されるため、学習速度が大幅に低下してしまいます。\\\n",
    "この時、再学習するネットワーク層を限定することで学習時のメモリ使用量を減らしながら、高速に学習を進めることが可能となります。\\\n",
    "全結合層(`fully-connected layer`)だけを再学習しても、なかなか成果につながりにくいため、ここでは[ResNet18](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)モデルの`fc`と`layer4`を再学習させることにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを凍結して使う場合は、全てのパラメータが持つ学習フラグを無効化します。デフォルトはrequires_grad = Trueで全てのパラメータを再学習します。\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定のネットワーク層の凍結を解除して再学習させるために、パラメータ名を確認します。\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全結合層(`fc`)と`layer4`を再学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fcとlayer4を再学習させます。fcは入れ替えているので必ず学習させてください。\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"layer4\"):\n",
    "        param.requires_grad = True\n",
    "    if name.startswith(\"fc\"):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再学習させるパラメータを確認します。\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、特定の層を選んで再学習させることもできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 視覚化ユーティリティ\n",
    "[bokeh](https://docs.bokeh.org/en/latest/docs/installation.html)を使って学習中の損失(loss)と精度(accuracy)をグラフに表示することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.layouts import row\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models.tickers import SingleIntervalTicker\n",
    "output_notebook()\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "p1 = figure(title=\"Loss\", x_axis_label=\"Epoch\", plot_height=300, plot_width=360)\n",
    "p2 = figure(title=\"Accuracy\", x_axis_label=\"Epoch\", plot_height=300, plot_width=360)\n",
    "\n",
    "source1 = ColumnDataSource(data={'epochs': [], 'trainlosses': [], 'testlosses': [] })\n",
    "source2 = ColumnDataSource(data={'epochs': [], 'train_accuracies': [], 'test_accuracies': []})\n",
    "\n",
    "#r = p1.multi_line(ys=['trainlosses', 'testlosses'], xs='epochs', color=colors, alpha=0.8, legend_label=['Training','Test'], source=source)\n",
    "r1 = p1.line(x='epochs', y='trainlosses', line_width=2, color=colors[0], alpha=0.8, legend_label=\"Train\", source=source1)\n",
    "r2 = p1.line(x='epochs', y='testlosses', line_width=2, color=colors[1], alpha=0.8, legend_label=\"Test\", source=source1)\n",
    "\n",
    "r3 = p2.line(x='epochs', y='train_accuracies', line_width=2, color=colors[0], alpha=0.8, legend_label=\"Train\", source=source2)\n",
    "r4 = p2.line(x='epochs', y='test_accuracies', line_width=2, color=colors[1], alpha=0.8, legend_label=\"Test\", source=source2)\n",
    "\n",
    "p1.legend.location = \"top_right\"\n",
    "p1.legend.click_policy=\"hide\"\n",
    "\n",
    "p2.legend.location = \"bottom_right\"\n",
    "p2.legend.click_policy=\"hide\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training(モデルの学習)\n",
    "\n",
    "20エポック学習し、各エポックで以前の最高精度と現在の精度を比較することにより、最高精度を更新した場合に保存します。\\\n",
    "現在の精度が以前の最高精度と等しい場合は、損失の少ない方を保存します。\n",
    "\n",
    "> 1エポックは、私たちが用意したトレーニング用のデータ全部を1回学習することです。一度に8枚の画像を学習するミニバッチ処理を複数回実行することで1エポックが完了します。\n",
    "\n",
    "**Collision Avoidance**の時は、「free(直進する)」or「blocked(旋回する)」それぞれに対する正解ラベルは`True`or`False`の 0 or 1 で定義できました。また、「free(直進する)」or「blocked(旋回する)」のうち、1つだけが`True`となるため**one hot value**として定義できました。**one hot value**を予測する場合、モデルの精度の定義はテストデータでの予測結果が、**「正解ラベルと一致している件数」÷「テストデータの総数」**となります。\n",
    "\n",
    "**Road Following**では正解ラベルはx,yの2出力それぞれ[-1.0,1.0]のfloat型の範囲になります。このため、正解ラベルと一致しない場合が多くなり、精度の定義は難しくなります。この場合、最低損失を更新した場合にモデルを保存します。\n",
    "\n",
    "今回は、Collision Avoidanceと同じような学習コードの構成にしたいので、何らかの方法で精度を定義したいとおもいます。\\\n",
    "ここでは`MSE`をlossとして定義しているため、`1.0 - RMSE`をaccuracyとして定義することにします。\n",
    "\n",
    "one hot valueの詳細：https://www.youtube.com/watch?v=v_4KWmkwmsU \\\n",
    "回帰モデルの評価指標の詳細：https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/#15.-Model-Evaluation-Metrics-for-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BEST_MODEL_PATH = 'best_steering_model_xy.pth'\n",
    "best_accuracy = 0.0\n",
    "saved_loss = 1e9\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "handle = show(row(p1, p2), notebook_handle=True)\n",
    "\n",
    "# create RMSELoss function\n",
    "def RMSELoss(yhat,y):\n",
    "    '''\n",
    "    yhat: predicted value\n",
    "    y: observed value\n",
    "    '''\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "criterion = RMSELoss\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_error_count = 0.0 # for graph\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        train_loss += float(loss)\n",
    "        train_error_count += criterion(outputs, labels) # for graph\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_error_count = 0.0 # for graph\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "        test_error_count += criterion(outputs, labels) # for graph\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    train_accuracy = 1.0 - float(train_error_count) / float(len(train_dataset)) # for graph\n",
    "    test_accuracy = 1.0 - float(test_error_count) / float(len(test_dataset)) # for graph\n",
    "\n",
    "    # 今回のepoch学習のテスト結果がよければ保存します\n",
    "    is_saved = False\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_accuracy = test_accuracy\n",
    "        saved_loss = test_loss\n",
    "        is_saved = True\n",
    "    elif test_accuracy == best_accuracy:\n",
    "        if test_loss < saved_loss:\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            saved_loss = test_loss\n",
    "            is_saved = True\n",
    "\n",
    "    print('%d: %f, %f, %f, %f, ' % (epoch+1, train_loss, test_loss, train_accuracy, test_accuracy)+(\"saved\" if is_saved else \"not saved\"))\n",
    "\n",
    "    # 学習状況をグラフに表示します\n",
    "    new_data1 = {'epochs': [epoch+1],\n",
    "                 'trainlosses': [float(train_loss)],\n",
    "                 'testlosses': [float(test_loss)] }\n",
    "    source1.stream(new_data1)\n",
    "    new_data2 = {'epochs': [epoch+1],\n",
    "                 'train_accuracies': [float(train_accuracy)],\n",
    "                 'test_accuracies': [float(test_accuracy)] }\n",
    "    source2.stream(new_data2)\n",
    "    push_notebook(handle=handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習が完了すると、``live_demo.ipynb``で推論に使う``best_steering_model_xy.pth``が生成されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next(次)\n",
    "\n",
    "次は、``live_demo.ipynb``を実行し、学習済みモデルで自動走行します。\\\n",
    "ノートブックメニューから`Kernel`->`Restert Kernel`を選んでJupyter kernelを再起動するか、JetBotを一度再起動してから次に進むとスムーズに進行できます。\n",
    "\n",
    "TensorRTを試したい人は、trtフォルダの中の``convert_to_trt.ipynb``を実行し、学習済みモデルをTensorRT形式に変換し、``live_demo_trt.ipynb``を実行し自動走行します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
