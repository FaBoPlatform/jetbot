{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following \n",
    "\n",
    "collision avoidanceを実行していれば、今回実行する次の3ステップには馴染みがあるでしょう。\n",
    "\n",
    "\n",
    "1. Data収集\n",
    "2. 学習\n",
    "3. デプロイ\n",
    "\n",
    "このnotebookでも、同じように実行していきます。ただし、今回はJetBotが道路（または実際に任意のパスまたはターゲットポイント）を走行できるようにするために、画像分類の代わりに、**回帰**という別の手法で学習させます。\n",
    "\n",
    " 1. JetBotをパス上のさまざまな位置に配置します（中心からのオフセット、さまざまな角度など）\n",
    "\n",
    "> データの多様性こそが重要であると、collision avoidanceのときに覚えたはずです。\n",
    "\n",
    "\n",
    "2. robotからのライブカメラの画像を画面に表示します\n",
    "3. マウスでX,Y座標を移動することで、`greenの点`をロボットが移動したい方向に移動します。\n",
    "4. この`greenの点`のX、Y座標の値をロボットのカメラからの画像と、一緒に保存します\n",
    "\n",
    "\n",
    "notebookの学習で、ラベルのX, Yの値の推論をおこなうためにneural networkで学習します。このライブデモでは、Jetbotのステアリングの値を計算するために予測されたX、Y値を使用します。(角度として「正確に」ではありません。画像のキャリブレーションが必要になりますが、角度にほぼ比例するため、制御は正常に機能します)\n",
    "\n",
    "それでは、このサンプルの目標点は、どこに配置するのがいいでしょうか？ 下記が役立つと思われるガイドです。\n",
    "\n",
    "1. カメラからのライブデモフィードを見ます\n",
    "2. ロボットがたどるべき経路を想像してください（道路からの脱輪などを回避するために必要な距離を計算してみてください）\n",
    "3. ロボットが道路を「脱輪」する事なく目標点にまっすぐ進むことができるように、目標点はこの経路に沿ったできるだけ遠くの位置に配置します。\n",
    "\n",
    "> 例として、もし真っ直ぐのロードがあったとします、目標点は地平線にします。もし、鋭く曲がったカーブなら、脱輪しないレベルで、ロボットの近くに配置する必要があるでしょう。\n",
    " \n",
    "ディープラーニングのモデルが意図したとおりに機能すると仮定すると、これらのラベリングガイドラインでは次のことが保証されます。\n",
    "\n",
    "1.ロボットは、ターゲットに向かって安全に直接移動できます（範囲外に出ることなく）。\n",
    "2.目標点は、想像した経路に沿って継続的に処理されます。\n",
    "\n",
    "この処理で得られるものは、望む軌道に沿って動く「スティック上のニンジン」です。ディープラーニングがニンジンを配置する場所を推論し、JetBotがそれに追従します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling example video(ラベリングのデモ動画)\n",
    "\n",
    "下記URLの動画を参考に、画像にラベルを付ける方法の例を確認します。このモデルはたった123枚の画像で動作しました:)\n",
    "\n",
    "YouTubeのサンプル動画\n",
    "\n",
    "https://www.youtube.com/embed/FW4En6LejhI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries(ライブラリのimport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、\"data collection\"のために必要なライブラリ全部をimportして、作業を開始します。ラベルと紐付いた画像を表示し、保存するためにOpenCVを使用します。uuid、datetimeなどのライブラリは、イメージのファイル名の命名に使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython Libraries for display and widgets\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Camera and Motor Interface for JetBot\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "\n",
    "# Python basic pakcages for image annotation\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Live Camera Feed(ライブカメラフィードの表示)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に、teleperation notebookで実行したようにカメラの初期化と表示をおこないます。\n",
    "\n",
    "JetBotのカメラクラスは、CSI MIPI cameraを有効にするために使います。neural networkでは、224x224ピクセスの画像データをインプットとして使います。データセットのファイルサイズを最小化するために、カメラをそのサイズに設定します(このタスクのために動作する動作する事は確認しています) いくつかのシナリオでは、画像サイズを大きくし収集し、後で目的のサイズに縮小する方がいいかもしれません。\n",
    "\n",
    "本サンプルを実行するにあたり、まずnvargus-daemon(カメラ等で使用)をリスタートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo jetbot | sudo -S systemctl restart nvargus-daemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = Camera.instance(width=224, height=224, fps=2)\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "target_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "\n",
    "x_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='x')\n",
    "y_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='y')\n",
    "\n",
    "def display_xy(camera_image):\n",
    "    image = np.copy(camera_image)\n",
    "    x = x_slider.value\n",
    "    y = y_slider.value\n",
    "    x = int(x * 224 / 2 + 112)\n",
    "    y = int(y * 224 / 2 + 112)\n",
    "    image = cv2.circle(image, (x, y), 8, (0, 255, 0), 3)\n",
    "    image = cv2.circle(image, (112, 224), 8, (0, 0,255), 3)\n",
    "    image = cv2.line(image, (x,y), (112,224), (255,0,0), 3)\n",
    "    jpeg_image = bgr8_to_jpeg(image)\n",
    "    return jpeg_image\n",
    "\n",
    "time.sleep(1)\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "traitlets.dlink((camera, 'value'), (target_widget, 'value'), transform=display_xy)\n",
    "\n",
    "display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data(データ収集)\n",
    "\n",
    "次のコードブロックは、ライブ画像フィードと保存した画像の数を表示します。ターゲットとなるX、Y値の値を下記のように保存します。\n",
    "\n",
    "\n",
    "1. 目標点に緑のドットを移動します。\n",
    "2. 保存ボタンを押します(本サンプル用に、オフィシャルのサンプルを修正しています。)\n",
    "\n",
    "\n",
    "これで、``dataset_xy``フォルダーに、\n",
    "\n",
    "``xy_<x value>_<y value>_<uuid>.jpg``\n",
    "\n",
    "の形式で保存されます。\n",
    "\n",
    "\n",
    "学習時に、ファイル名からx,yの値をパースし、イメージとともに読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'dataset_xy'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(DATASET_DIR)\n",
    "except FileExistsError:\n",
    "    print('Directories not created becasue they already exist')\n",
    "\n",
    "#for b in controller.buttons:\n",
    "#    b.unobserve_all()\n",
    "\n",
    "count_widget = widgets.IntText(description='count', value=len(glob.glob(os.path.join(DATASET_DIR, '*.jpg'))))\n",
    "\n",
    "def xy_uuid(x, y):\n",
    "    return 'xy_%03d_%03d_%s' % (x * 50 + 50, y * 50 + 50, uuid1())\n",
    "\n",
    "def save_snapshot():\n",
    "    #if change['new']:\n",
    "    uuid = xy_uuid(x_slider.value, y_slider.value)\n",
    "    image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(image_widget.value)\n",
    "        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "\n",
    "#controller.buttons[13].observe(save_snapshot, names='value')\n",
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "add_button = widgets.Button(description='追加', button_style='success', layout=button_layout)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    target_widget,\n",
    "    x_slider,\n",
    "    y_slider,\n",
    "    count_widget,\n",
    "    add_button\n",
    "]))\n",
    "\n",
    "add_button.on_click(lambda x: save_snapshot())\n",
    "\n",
    "#display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next(次)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、train_mlodel.ipynbで学習をおこないます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
