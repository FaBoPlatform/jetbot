{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following \n",
    "\n",
    "collision avoidanceを実行していれば、今回のこの3ステップには馴染みがあるでしょう。\n",
    "\n",
    "\n",
    "1. Data収集\n",
    "2. 学習\n",
    "3. デプロイ\n",
    "\n",
    "このnotebookは、同じことをします。ただし、今回はJetBotが道路（または実際に任意のパスまたはターゲットポイント）をたどることができるようにするために、画像分類の代わりに、**回帰**という別の基本的な手法で学習します。\n",
    "\n",
    " 1. JetBotをパス上のさまざまな位置に配置します（中心からのオフセット、さまざまな角度など）\n",
    "\n",
    "> データのバリデーションこそが重要であると、collision avoidanceのときに覚えたはずです。\n",
    "\n",
    "\n",
    "2. robotからのライブカメラフィードを画面に表示します\n",
    "3. ゲームパッドを使い(改良して本サンプルはマウスのみ)、`greenの点`を画像上でロボットが移動するターゲットの方向に移動します。\n",
    "4. この`greenの点`のX、Y値をロボットのカメラからの画像と、一緒に保存します\n",
    "\n",
    "\n",
    "notebookの学習で、ラベルのX, Yの値の予測をおこなうためにneural networkで学習します。このライブデモでは、概算のステアリング値を計算するために予測されたX、Y値を使用します。(角度として「正確に」ではありません。画像のキャリブレーションが必要になりますが、角度にほぼ比例するため、制御は正常に機能します)\n",
    "\n",
    "\n",
    "それでは、この例の目標点は、正確にどこに配置するのがいいでしょうか？ ここに役立つと思われるガイドがあります。\n",
    "\n",
    "\n",
    "1. カメラからのライブデモフィードを見ます\n",
    "2. ロボットがたどるべき経路を想像してください（道路からの脱出などを回避するために必要な距離を概算してみてください）\n",
    "3. ロボットが道路を「脱線」する事なく目標点にまっすぐ進むことができるように、目標点はこの経路に沿ってできるだけ遠くに配置します。\n",
    "\n",
    "\n",
    "> 例として、もし真っ直ぐのロードがあったとします、目標点は地平線にします。もし、鋭く曲がったカーブなら、脱線しないレベルで、ロボットの近くに配置する必要があるでしょう。\n",
    "\n",
    " \n",
    "ディープラーニングのモデルが意図したとおりに機能すると仮定すると、これらのラベリングガイドラインでは次のことが保証されます。\n",
    "\n",
    "\n",
    "1.ロボットは、ターゲットに向かって安全に直接移動できます（範囲外に出ることなく）。\n",
    "2.目標点は、想像した経路に沿って継続的に処理されます。\n",
    "\n",
    "\n",
    "この処理で得られるものは、望む軌道に沿って動く「スティック上のニンジン」です。ディープラーニングがニンジンを配置する場所を決定し、JetBotがそれに追従します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling example video(ラベリングのデモ動画)\n",
    "\n",
    "下記URLの動画を参考に、画像にラベルを付ける方法の例を確認します。このモデルはたった123枚の画像で動作しました:)\n",
    "\n",
    "https://www.youtube.com/embed/FW4En6LejhI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries(ライブラリのimport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、\"data collection\"のために必要なライブラリ全部をimportして、作業を開始します。ラベルと紐付いた画像を表示し、保存するためにOpenCVを使用します。uuid、datetimeなどのライブラリは、イメージのファイル名の命名に使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython Libraries for display and widgets\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Camera and Motor Interface for JetBot\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "\n",
    "# Python basic pakcages for image annotation\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Live Camera Feed(ライブカメラフィードの表示)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に、teleperation notebookで実行したようにカメラの初期化と表示をおこないます。\n",
    "\n",
    "JetBotのカメラクラスは、CSI MIPI cameraを有効にするために使います。neural networkでは、224x224ピクセスの画像データをインプットとして使います。データセットのファイルサイズを最小化するために、カメラをそのサイズに設定します(このタスクのために動作する動作する事は確認しています) いくつかのシナリオでは、画像サイズを大きくし収集し、後で目的のサイズに縮小する方がいいかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = Camera()\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "target_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "\n",
    "x_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='x')\n",
    "y_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='y')\n",
    "\n",
    "def display_xy(camera_image):\n",
    "    image = np.copy(camera_image)\n",
    "    x = x_slider.value\n",
    "    y = y_slider.value\n",
    "    x = int(x * 224 / 2 + 112)\n",
    "    y = int(y * 224 / 2 + 112)\n",
    "    image = cv2.circle(image, (x, y), 8, (0, 255, 0), 3)\n",
    "    image = cv2.circle(image, (112, 224), 8, (0, 0,255), 3)\n",
    "    image = cv2.line(image, (x,y), (112,224), (255,0,0), 3)\n",
    "    jpeg_image = bgr8_to_jpeg(image)\n",
    "    return jpeg_image\n",
    "\n",
    "time.sleep(1)\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "traitlets.dlink((camera, 'value'), (target_widget, 'value'), transform=display_xy)\n",
    "\n",
    "display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data(データ収集)\n",
    "\n",
    "次のコードブロックは、ライブ画像フィードと保存した画像の数を表示します。ターゲットとなるX、Y値の値を下記のように保存します。\n",
    "\n",
    "\n",
    "1. 目標点に緑のドットを移動します。\n",
    "2. 保存ボタンを押します(本サンプル用に、オフィシャルのサンプルを修正しています。)\n",
    "\n",
    "\n",
    "これで、``dataset_xy``フォルダーに、\n",
    "\n",
    "``xy_<x value>_<y value>_<uuid>.jpg``\n",
    "\n",
    "の形式で保存されます。\n",
    "\n",
    "\n",
    "学習時に、ファイル名からx,yの値をパースし、イメージとともに読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'dataset_xy'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(DATASET_DIR)\n",
    "except FileExistsError:\n",
    "    print('Directories not created becasue they already exist')\n",
    "\n",
    "#for b in controller.buttons:\n",
    "#    b.unobserve_all()\n",
    "\n",
    "count_widget = widgets.IntText(description='count', value=len(glob.glob(os.path.join(DATASET_DIR, '*.jpg'))))\n",
    "\n",
    "def xy_uuid(x, y):\n",
    "    return 'xy_%03d_%03d_%s' % (x * 50 + 50, y * 50 + 50, uuid1())\n",
    "\n",
    "def save_snapshot():\n",
    "    #if change['new']:\n",
    "    uuid = xy_uuid(x_slider.value, y_slider.value)\n",
    "    image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(image_widget.value)\n",
    "        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "\n",
    "#controller.buttons[13].observe(save_snapshot, names='value')\n",
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "add_button = widgets.Button(description='add', button_style='success', layout=button_layout)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    target_widget,\n",
    "    x_slider,\n",
    "    y_slider,\n",
    "    count_widget,\n",
    "    add_button\n",
    "]))\n",
    "\n",
    "add_button.on_click(lambda x: save_snapshot())\n",
    "\n",
    "#display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next(次)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
